# Meta Lingua

**Mathurin Videau***, **Badr Youbi Idrissi***, Daniel Haziza, Luca Wehrstedt, Jade Copet, Olivier Teytaud, David Lopez-Paz. ***Equal and main contribution**

Meta Lingua is a minimal and fast LLM training and inference library designed for research. Meta Lingua uses easy-to-modify PyTorch components in order to try new architectures, losses, data, etc. We aim for this code to enable end to end training, inference and evaluation as well as provide tools to better understand speed and stability. While Meta Lingua is currently under development, we provide you with multiple `apps` to showcase how to use this codebase.

<p align="center">  
 <img src="lingua_overview.svg" width="100%"/>
</p>

## Quick start

The following commands launch a SLURM job that creates an environment for Meta Lingua.
The env creation should take around 5 minutes without counting downloads. 

```bash
git clone https://github.com/facebookresearch/lingua
cd lingua

bash setup/create_env.sh
# or if you have access to a SLURM cluster
sbatch setup/create_env.sh
```
Once that is done your can activate the environment 
```bash
conda activate lingua_<date>
```
use the provided script to download and prepare data from huggingface (among `fineweb_edu`, `fineweb_edu_10bt`, or `dclm_baseline_1.0`).
This command will download the `fineweb_edu` and prepare it for training in the `./data` directory, specifying the amount of memory `terashuf` (the tool used to shuffle samples) will be allocated. By default, the number of chunks (`nchunks`) is 32. If you are running on fewer than 32 GPUs, it is recommended to set `nchunks` to 1 or to match `nchunks` with the number of GPUs (`nchunks` = NGPUs). See [here](https://github.com/facebookresearch/lingua/issues/55#issuecomment-2483643076) for more details.
```bash
python setup/download_prepare_hf_data.py fineweb_edu <MEMORY> --data_dir ./data --seed 42 --nchunks <NCHUNKS>
```
to download tokenizer (here llama3), use the folowing script:
```bash
python setup/download_tokenizer.py llama3 <SAVE_PATH> --api_key <HUGGINGFACE_TOKEN>
```
Now launch a debug job to check if everything works.  **The provided configurations are templates, you need to adapt them for them to work (change `dump_dir`, `data.root_dir`, `data.tokenizer.path`, etc ...)**

```bash
# stool stands for SLURM tool !
python -m lingua.stool script=apps.main.train config=apps/main/configs/debug.yaml nodes=1 partition=<partition>
# if you want to launch locally you can use torchrun
torchrun --nproc-per-node 8 -m apps.main.train config=apps/main/configs/debug.yaml
# or you can also launch on 1 GPU
python -m apps.main.train config=apps/main/configs/debug.yaml
```

When using `stool`, if a job crashes, it can be relaunched using sbatch:
```bash
sbatch path/to/dump_dir/submit.slurm
```
## è®­ç»ƒç»“æœ

æˆ‘ä»¬åœ¨è®¸å¤šä¸‹æ¸¸ä»»åŠ¡ä¸­è·å¾—äº†éå¸¸å¼ºçš„æ€§èƒ½ï¼Œå¹¶åŒ¹é…äº†[DCLM baseline 1.0](https://arxiv.org/abs/2406.11794)çš„æ€§èƒ½ã€‚

### 1Bæ¨¡å‹åœ¨60B DCLM tokensä¸Šçš„è¡¨ç°
| name           | arc_challenge | arc_easy | boolq |  copa | hellaswag |  obqa |  piqa |  siqa | winogrande |  nq  |  tqa  |
|----------------|:-------------:|:--------:|:-----:|:-----:|:---------:|:-----:|:-----:|:-----:|:----------:|:----:|:-----:|
| Transformer 1B |     36.48     |   62.83  | 62.57 | 79.00 |   63.62   | 37.40 | 75.14 | 45.19 |    61.64   | 8.75 | 26.31 |
| minGRU 1B      |     30.82     |   57.89  | 62.05 | 74.00 |   50.27   | 37.00 | 72.31 | 43.76 |    52.49   | 3.24 |  9.03 |
| minLSTM 1B     |     31.76     |   60.04  | 62.02 | 73.00 |   53.39   | 36.40 | 72.36 | 45.09 |    52.80   | 4.52 | 12.73 |
| Hawk 1B        |     34.94     |   63.68  | 62.42 | 76.00 |   63.10   | 38.20 | 73.23 | 46.01 |    55.33   | 8.42 | 23.58 |
| Mamba 1B       |     35.54     |   63.42  | 62.63 | 74.00 |   64.16   | 38.80 | 75.24 | 45.14 |    60.14   | 8.84 | 26.64 |

### 7B models

| name                             | arc_challenge | arc_easy | boolq | copa  | hellaswag | obqa  | piqa  | siqa  | winogrande | mmlu  | nq    | tqa   | bbh   |
|----------------------------------|---------------|----------|-------|-------|-----------|-------|-------|-------|------------|-------|-------|-------|-------|
| Mamba 7B 200B tokens             | 47.21         | 76.03    | 65.63 | 84.00 | 77.80     | 44.00 | 80.25 | 49.69 | 70.24      | 32.81 | 20.53 | 51.93 | 20.35 |
| Llama 7B 200B tokens             | 46.95         | 75.73    | 64.80 | 84.00 | 77.45     | 45.00 | 80.20 | 48.26 | 70.32      | 48.64 | 20.66 | 51.01 | 31.47 |
| Llama 7B squared relu 1T tokens  | 49.61         | 76.74    | 72.45 | 89.00 | 81.19     | 44.80 | 82.05 | 49.95 | 72.14      | 60.56 | 25.68 | 59.52 | 42.11 |

## Project overview

Meta Lingua is structured as follows:

```
ğŸ“¦meta-lingua
 â”£ ğŸ“‚lingua # Core library
 â”ƒ â”£ ğŸ“œargs.py
 â”ƒ â”£ ğŸ“œcheckpoint.py
 â”ƒ â”£ ğŸ“œdata.py
 â”ƒ â”£ ğŸ“œdistributed.py
 â”ƒ â”£ ğŸ“œfloat8.py
 â”ƒ â”£ ğŸ“œlogger.py
 â”ƒ â”£ ğŸ“œmetrics.py
 â”ƒ â”£ ğŸ“œoptim.py
 â”ƒ â”£ ğŸ“œprobe.py
 â”ƒ â”£ ğŸ“œprofiling.py
 â”ƒ â”£ ğŸ“œstool.py
 â”ƒ â”£ ğŸ“œtokenizer.py
 â”ƒ â”— ğŸ“œtransformer.py
 â”£ ğŸ“‚setup
 â”ƒ â”£ ğŸ“œcreate_env.sh
 â”ƒ â”— ğŸ“œdownload_prepare_hf_data.py
 â”— ğŸ“‚apps # Apps that put components together
   â”£ ğŸ“‚main # Main language modeling app with llama
   â”ƒ â”£ ğŸ“‚configs
   â”ƒ â”£ ğŸ“œeval.py
   â”ƒ â”£ ğŸ“œgenerate.py
   â”ƒ â”£ ğŸ“œtrain.py
   â”ƒ â”— ğŸ“œtransformer.py
   â”£ ğŸ“‚fastRNN 
   â”ƒ â”£ ğŸ“‚component
   â”ƒ â”£ ğŸ“‚hawk
   â”ƒ â”£ ğŸ“‚minGRU
   â”ƒ â”£ ğŸ“‚minLSTM
   â”£ ğŸ“‚mamba
   â”£ ğŸ“‚mtp # Multi token prediction
   â”— ğŸ“‚plots
```

`lingua`æ–‡ä»¶å¤¹åŒ…å«ä¸€äº›åŸºæœ¬ä¸”å¯é‡ç”¨çš„ç»„ä»¶ï¼Œè€Œ`apps`æ–‡ä»¶å¤¹åŒ…å«å°†è¿™äº›ç»„ä»¶ç»„åˆåœ¨ä¸€èµ·çš„è„šæœ¬ã€‚ä¾‹å¦‚ï¼Œä¸»è¦çš„è®­ç»ƒå¾ªç¯ä½äº`apps/main`ä¸­ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨å°†å…¶ä½œä¸ºæ¨¡æ¿ï¼Œå¹¶æ ¹æ®æ‚¨çš„å®éªŒéœ€æ±‚éšæ„ä¿®æ”¹ã€‚

åœ¨Meta Linguaä¸­æ²¡æœ‰ä»€ä¹ˆæ˜¯ä¸å¯æ”¹å˜çš„ã€‚æˆ‘ä»¬ç‰¹æ„å°è¯•ä½¿å…¶å°½å¯èƒ½æ˜“äºä¿®æ”¹ï¼æ‰€ä»¥è¯·éšæ„åˆ†æ”¯å¹¶ä¿®æ”¹ä»»ä½•å†…å®¹ã€‚

ä»¥ä¸‹æ˜¯æœ€é‡è¦æ–‡ä»¶å’ŒåŠŸèƒ½çš„ç®€è¦æè¿°ï¼š

- **`transformer.py`**ï¼šå®šä¹‰æ¨¡å‹æ¶æ„ã€‚è¿™æ˜¯çº¯PyTorch `nn.Module`ï¼è¿™é‡Œæ²¡æœ‰ä»€ä¹ˆèŠ±å“¨çš„ä¸œè¥¿ã€‚
- **`distributed.py`**ï¼šå¤„ç†åœ¨å¤šä¸ªGPUä¸Šåˆ†å¸ƒæ¨¡å‹ã€‚è¿™æ˜¯é€šè¿‡`parallelize_module`å‡½æ•°å®Œæˆçš„ï¼Œè¯¥å‡½æ•°åŒ…è£…æ‚¨çš„æ™®é€š`nn.Module`å¹¶åº”ç”¨å‡ ä¹ä»»ä½•æ•°æ®å¹¶è¡Œã€å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œã€`torch.compile`ã€æ¿€æ´»æ£€æŸ¥ç‚¹å’Œ`float8`çš„ç»„åˆã€‚
- **`data.py`**ï¼šLLMé¢„è®­ç»ƒçš„æ•°æ®åŠ è½½å™¨ã€‚

<p align="center">  
 <img src="dataloader.png" width="40%"/>
</p>

- **`profiling.py`**ï¼šxformersåˆ†æå™¨çš„å°åŒ…è£…å™¨ï¼Œæä¾›è‡ªåŠ¨MFUå’ŒHFUè®¡ç®—ï¼Œå¹¶åœ¨è½¬å‚¨ç›®å½•çš„åˆ†ææ–‡ä»¶å¤¹ä¸­è½¬å‚¨åˆ†æè·Ÿè¸ªã€‚å®ƒè¿˜å…·æœ‰å†…å­˜åˆ†æè·Ÿè¸ªã€‚
- **`checkpoint.py`**ï¼šç®¡ç†æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚å®ƒå°†æ¨¡å‹ä¿å­˜åœ¨è½¬å‚¨ç›®å½•çš„checkpointsæ–‡ä»¶å¤¹ä¸­ï¼Œé‡‡ç”¨.distcpæ ¼å¼ï¼Œè¿™æ˜¯æ–°çš„PyTorchåˆ†å¸ƒå¼ä¿å­˜æ–¹æ³•ã€‚æ­¤æ ¼å¼å…è®¸ä½¿ç”¨ä¸åŒæ•°é‡çš„GPUå’Œä¸åŒçš„åˆ†ç‰‡é‡æ–°åŠ è½½æ¨¡å‹ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`torch.distributed.checkpoint.format_utils.dcp_to_torch_save`å°†å®ƒä»¬è½¬æ¢ä¸ºæ™®é€šPyTorchæ£€æŸ¥ç‚¹ï¼Œåä¹‹äº¦ç„¶ï¼Œä½¿ç”¨`torch_save_to_dcp`ã€‚
- **`args.py`**ï¼šç”¨äºå¤„ç†é…ç½®çš„å®ç”¨å·¥å…·ã€‚

## é…ç½®

å¤§å¤šæ•°ç»„ä»¶éœ€è¦é…ç½®ï¼Œæˆ‘ä»¬é€‰æ‹©ä½¿ç”¨æ•°æ®ç±»æ¥è¡¨ç¤ºè¿™äº›é…ç½®å¯¹è±¡ã€‚`args.py`å¸®åŠ©åœ¨`config.yaml`å’Œé…ç½®å­—å…¸ä¹‹é—´è¿›è¡Œè½¬æ¢ï¼Œåˆ†åˆ«è½¬æ¢ä¸ºå„è‡ªçš„æ•°æ®ç±»ã€‚

ä¾‹å¦‚ï¼Œ`apps/main/train.py`ä¸­çš„`TrainArgs`å…·æœ‰`LMTransformerArgs`ã€`OptimArgs`ç­‰å­ç±»ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå°†è½¬æ¢ä¸º`TrainArgs`çš„ç¤ºä¾‹é…ç½®æ–‡ä»¶ï¼š

```yaml
# è¿™æ˜¯Meta Linguaå°†å­˜å‚¨ä¸å®éªŒç›¸å…³çš„ä»»ä½•å†…å®¹çš„ä½ç½®ã€‚
dump_dir: /path/to/dumpdir
name: "debug"
steps: 1000

seed: 12

optim:
    lr: 3e-4
    warmup: 2000
    lr_min_ratio: 0.000001
    clip: 10.0

distributed:
    fsdp_type: full_shard
    compile: true
    selective_activation_checkpointing: false

model:
    dim: 1024
    n_layers: 8
    n_heads: 8

data:
    root_dir: data/shuffled
    sources:
      wikipedia: 80.0
      arxiv: 20.0
    batch_size: 32
    seq_len: 1024
    load_async: true
    tokenizer:
        name: sp
        path: tokenizers/llama2.model
```


## Launching jobs

### Command line arguments

æ‰€æœ‰è„šæœ¬ï¼ˆ`train.py`ã€`eval.py`ã€`stool.py`ï¼‰ä¸­çš„å‘½ä»¤è¡Œæ¥å£ä½¿ç”¨[OmegaConf](https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#from-command-line-arguments)
å®ƒæ¥å—ç‚¹åˆ—è¡¨å½¢å¼çš„å‚æ•°
å¦‚æœæ•°æ®ç±»å¦‚ä¸‹æ‰€ç¤º
```python
@dataclass
class DummyArgs:
    name: str = "blipbloup"
    mode: LMTransformerArgs = LMTransformerArgs()
    
@dataclass
class LMTransformerArgs:
    dim: int = 512
    n_layers: int = 12
```

Then you can pass `model.dim = 32` to change values in `LMTransformerArgs`
or just `name = tictac` for top level attributes.

**`train.py`** ç®€å•åœ°æ¥å—ä½œä¸ºå‚æ•°çš„é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¹¶åŠ è½½è¯¥é…ç½®ã€‚è¡Œä¸ºå¦‚ä¸‹ï¼š
1. æˆ‘ä»¬ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–`TrainArgs`
2. æˆ‘ä»¬ç”¨æä¾›çš„é…ç½®æ–‡ä»¶ä¸­çš„å€¼è¦†ç›–é‚£äº›é»˜è®¤å€¼
3. æˆ‘ä»¬ç”¨é€šè¿‡å‘½ä»¤è¡Œæä¾›çš„é¢å¤–å‚æ•°è¦†ç›–ç»“æœ

If we take the `DummyArgs` example above, calling `train.py` with `train.py config=debug.yaml model.dim=64 name=tictac` 
where `debug.yaml` contains 
```yaml
model:
    n_layers: 24
```
å°†å¯åŠ¨è®­ç»ƒï¼Œä½¿ç”¨é…ç½®
```python
DummyArgs(name="tictac", LMTransformerArgs(dim=64, n_layers=24))
```

### ä½¿ç”¨SLURMå¯åŠ¨

ç”±äºæˆ‘ä»¬æƒ³è¦è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦`train.py`è¿è¡ŒNæ¬¡ï¼ˆNæ˜¯GPUçš„æ•°é‡ï¼‰

æœ€ç®€å•çš„æ–¹æ³•æ˜¯é€šè¿‡SLURMã€‚ä¸ºäº†ç®€åŒ–è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æä¾›äº†`lingua/stool.py`ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„pythonè„šæœ¬ï¼Œ
1. å°†æä¾›çš„é…ç½®ä¿å­˜åˆ°`dump_dir`
2. å°†å½“å‰ä»£ç å¤åˆ¶åˆ°`dump_dir`ä»¥å¤‡ä»½
3. åˆ›å»ºä¸€ä¸ªsbatchæ–‡ä»¶`submit.slurm`ï¼Œç„¶åä½¿ç”¨æä¾›çš„é…ç½®å¯åŠ¨ä»»åŠ¡ã€‚

å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œä½¿ç”¨å®ƒ

```bash
python -m lingua.stool config=apps/main/configs/debug.yaml nodes=1 account=fair_amaia_cw_codegen qos=lowest
```

æˆ–è€…ç›´æ¥ä½¿ç”¨`launch_job`å‡½æ•°ã€‚è¿™å…è®¸æ‚¨ä¾‹å¦‚åœ¨jupyter notebookä¸­åˆ›å»ºè®¸å¤šä»»æ„é…ç½®ï¼ˆç”¨äºå‚æ•°æ‰«æã€è¿›è¡Œæ¶ˆèï¼‰ï¼Œå¹¶ç›´æ¥ä»é‚£é‡Œå¯åŠ¨ä»»åŠ¡ã€‚

ç”±äºé…ç½®æ–‡ä»¶è¢«å¤åˆ¶åˆ°`dump_dir`ï¼Œä¸€ä¸ªç®€å•çš„è¿­ä»£æ–¹æ³•æ˜¯ç®€å•åœ°æ›´æ”¹é…ç½®æ–‡ä»¶å¹¶é‡æ–°å¯åŠ¨ç›¸åŒçš„å‘½ä»¤ã€‚

## è°ƒè¯•
ä¸ºäº†å¿«é€Ÿè¿­ä»£ï¼Œæœ€å¥½ä¸å¿…æ¯æ¬¡éƒ½ç­‰å¾…SLURMåˆ†é…ã€‚æ‚¨å¯ä»¥æ”¹ä¸ºè¯·æ±‚SLURMä¸ºæ‚¨åˆ†é…èµ„æºï¼Œä¸€æ—¦åˆ†é…å®Œæˆï¼Œæ‚¨å¯ä»¥åœ¨åŒä¸€åˆ†é…ä¸Šè¿è¡Œå¤šä¸ªå‘½ä»¤ã€‚

ä¾‹å¦‚æ‚¨å¯ä»¥è¿™æ ·åšï¼š

```bash
salloc --nodes 2 --cpus-per-gpu 16 --mem 1760GB --gres=gpu:8 --exclusive --time=72:00:00
```

è¿™å°†ä¸ºæ‚¨åœ¨å½“å‰ç»ˆç«¯ä¸­æä¾›2ä¸ªèŠ‚ç‚¹ã€‚ä¸€æ—¦åˆ†é…å®Œæˆï¼Œæ‚¨å°†çœ‹åˆ°ä¸€äº›è‡ªåŠ¨æ·»åŠ çš„SLURMç¯å¢ƒå˜é‡ï¼Œä¾‹å¦‚`$SLURM_JOB_ID`ç­‰ã€‚è¿™å…è®¸æ‚¨ä¾‹å¦‚åœ¨åŒä¸€ç»ˆç«¯ä¸­æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
srun -n 16 python -m apps.main.train config=apps/main/configs/debug.yaml
```

è¿™å°†è¿è¡Œ`python -m apps.main.train config=apps/main/configs/debug.yaml`å‘½ä»¤åœ¨æ¯ä¸ª16ä¸ªGPUä¸Šã€‚å¦‚æœè¿™ä¸ªå´©æºƒæˆ–ç»“æŸï¼Œæ‚¨å¯ä»¥ç®€å•åœ°é‡æ–°å¯åŠ¨`srun`ï¼Œå› ä¸ºèŠ‚ç‚¹å·²ç»åˆ†é…ç»™æ‚¨ï¼Œæ‚¨ä¸å¿…ç­‰å¾…SLURMå†æ¬¡ä¸ºæ‚¨æä¾›èµ„æºã€‚

This will also show you the outputs of all those commands in the same terminal which might become cumbersome. 

Instead you can use `stool` directly to configure logs to be separated into different files per GPU.

```bash
python -m lingua.stool config=apps/main/configs/debug.yaml nodes=2 launcher=bash dirs_exists_ok=true
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ·»åŠ äº† **`launcher=bash`**ï¼Œè¿™åŸºæœ¬ä¸Šæ„å‘³ç€ç”Ÿæˆçš„ `submit.slurm` å°†ç›´æ¥æ‰§è¡Œï¼Œè€Œä¸æ˜¯é€šè¿‡ `sbatch` æäº¤ã€‚`submit.slurm` ä¸­ä¹Ÿæœ‰ä¸€ä¸ª `srun` å‘½ä»¤ï¼Œæ‰€ä»¥è¿™ä¸ä¸Šé¢çš„ `srun` å‘½ä»¤éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬è¿˜æ·»åŠ äº† **`dirs_exists_ok=true`** æ¥å‘Šè¯‰ `stool` å¯ä»¥è¦†ç›–ç°æœ‰æ–‡ä»¶å¤¹ä¸­çš„å†…å®¹ï¼ˆä»£ç ã€é…ç½®ç­‰ï¼‰ã€‚

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ `pdb` é€æ­¥è°ƒè¯•ä»£ç ï¼Œåº”è¯¥ä½¿ç”¨ `-n 1` åªåœ¨1ä¸ªGPUä¸Šè¿è¡Œã€‚

## Evaluations

Evaluations can run either during training periodically or you directly launch evals on a given checkpoint as follows:

```bash
srun -n 8 python -u -m apps.main.eval config=apps/main/configs/eval.yaml
```

You need to specify the checkpoint and dump dir of the evaluation in that config

Or through `stool` with

```bash
python -m lingua.stool script=apps.main.eval config=apps/main/configs/eval.yaml nodes=1 account=fair_amaia_cw_codegen qos=lowest
```

## Dump dir structure

```
ğŸ“‚example_dump_dir
 â”£ ğŸ“‚checkpoints
 â”ƒ â”£ ğŸ“‚0000001000
 â”ƒ â”£ ğŸ“‚0000002000
 â”ƒ â”£ ğŸ“‚0000003000
 â”ƒ â”£ ğŸ“‚0000004000
 â”ƒ â”£ ğŸ“‚0000005000
 â”ƒ â”£ ğŸ“‚0000006000
 â”ƒ â”£ ğŸ“‚0000007000 # Checkpoint and train state saved every 1000 steps here
 â”ƒ â”ƒ â”£ ğŸ“œ.metadata
 â”ƒ â”ƒ â”£ ğŸ“œ__0_0.distcp
 â”ƒ â”ƒ â”£ ğŸ“œ__1_0.distcp
 â”ƒ â”ƒ â”£ ğŸ“œparams.json
 â”ƒ â”ƒ â”£ ğŸ“œtrain_state_00000.json
 â”ƒ â”ƒ â”— ğŸ“œtrain_state_00001.json
 â”£ ğŸ“‚code # Backup of the code at the moment the job was launched
 â”£ ğŸ“‚logs
 â”ƒ â”— ğŸ“‚166172 # Logs for each GPU in this SLURM job.
 â”ƒ â”ƒ â”£ ğŸ“œ166172.stderr
 â”ƒ â”ƒ â”£ ğŸ“œ166172.stdout
 â”ƒ â”ƒ â”£ ğŸ“œ166172_0.err
 â”ƒ â”ƒ â”£ ğŸ“œ166172_0.out
 â”ƒ â”ƒ â”£ ğŸ“œ166172_1.err
 â”ƒ â”ƒ â”— ğŸ“œ166172_1.out
 â”£ ğŸ“‚profiling
 â”ƒ â”£ ğŸ“‚memory_trace_plot # Trace of memory usage through time for all GPUs
 â”ƒ â”ƒ â”£ ğŸ“œ000102_h100-192-145_451082.html
 â”ƒ â”ƒ â”£ ğŸ“œ000102_h100-192-145_451083.html
 â”ƒ â”— ğŸ“‚profile_CPU_CUDA_000104 # Profiling traces for all GPUs
 â”ƒ â”ƒ â”£ ğŸ“œh100-192-145_451082.1720183858874741723.pt.trace.json.gz
 â”ƒ â”ƒ â”— ğŸ“œh100-192-145_451083.1720183858865656716.pt.trace.json.gz
 â”£ ğŸ“œbase_config.yaml
 â”£ ğŸ“œconfig.yaml
 â”£ ğŸ“œmetrics.jsonl
 â”— ğŸ“œsubmit.slurm
```

## Related repositories

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¼ºè°ƒä¸€äº›ä¸æœ¬é¡¹ç›®äº’è¡¥çš„ç›¸å…³å·¥ä½œã€‚å…¶ä¸­æœ€é‡è¦çš„æ˜¯[torchtitan](https://github.com/pytorch/torchtitan)å’Œ[torchtune](https://github.com/pytorch/torchtune)ã€‚

Linguaä¸“ä¸ºé‚£äº›æƒ³è¦å°è¯•LLMé¢„è®­ç»ƒæ–°æƒ³æ³•å¹¶å¿«é€Ÿè·å¾—è®­ç»ƒ/æ¨ç†é€Ÿåº¦å’Œä¸‹æ¸¸åŸºå‡†æµ‹è¯•åé¦ˆçš„ç ”ç©¶äººå‘˜è®¾è®¡ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡æä¾›è½»é‡çº§å’Œä¸“æ³¨çš„ä»£ç åº“ï¼Œé™ä½LLMç ”ç©¶çš„å…¥é—¨é—¨æ§›ã€‚

æˆ‘ä»¬å°†torchtitanã€torchtuneå’Œlinguaè§†ä¸ºäº’è¡¥å·¥å…·ã€‚Torchtitanéå¸¸é€‚åˆå¤§è§„æ¨¡å·¥ä½œï¼Œå› ä¸ºå®ƒå…·æœ‰3Då¹¶è¡Œæ€§ï¼Œå¹¶ä¸”ç”±äºä¸PyTorchå›¢é˜Ÿçš„ç´§å¯†è”ç³»ï¼Œå¯èƒ½ä¼šæ›´å¿«åœ°é›†æˆæœ€æ–°çš„PyTorchåˆ†å¸ƒå¼è®­ç»ƒåŠŸèƒ½ã€‚å¦ä¸€æ–¹é¢ï¼ŒTorchtuneåœ¨å¾®è°ƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨GPUèµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œå®ƒæä¾›äº†å„ç§å¾®è°ƒç­–ç•¥ï¼Œå¦‚LoRAã€QLoRAã€DPOå’ŒPPOã€‚

ä¸€ä¸ªå…¸å‹çš„å·¥ä½œæµç¨‹å¯èƒ½æ˜¯è¿™æ ·çš„ï¼šä½ å¯èƒ½é¦–å…ˆåœ¨Linguaä¸­æµ‹è¯•æ–°æƒ³æ³•ï¼Œç„¶åä½¿ç”¨Torchtitanè¿›ä¸€æ­¥æ‰©å±•ï¼Œæœ€åä½¿ç”¨Torchtuneè¿›è¡ŒæŒ‡ä»¤æˆ–åå¥½å¾®è°ƒã€‚

è™½ç„¶è¿™äº›ä»£ç åº“ä¹‹é—´è‚¯å®šæœ‰ä¸€äº›é‡å ï¼Œä½†æˆ‘ä»¬è®¤ä¸ºä¸ºLLMå·¥ä½œçš„ä¸åŒæ–¹é¢æä¾›ä¸“æ³¨çš„å·¥å…·æ˜¯æœ‰ä»·å€¼çš„ã€‚ä¾‹å¦‚ï¼ŒTorchtitanæ—¨åœ¨ä»¥å¹²å‡€ã€ç®€æ´çš„ä»£ç åº“å±•ç¤ºPyTorchæœ€æ–°çš„åˆ†å¸ƒå¼è®­ç»ƒåŠŸèƒ½ï¼Œä½†å¯¹äºå¤§å¤šæ•°ç ”ç©¶æ¥è¯´ï¼Œä½ çœŸçš„ä¸éœ€è¦PyTorchæä¾›çš„æ¯ä¸€ä¸ªåŠŸèƒ½ï¼Œæˆ–è€…åœ¨4096ä¸ªGPUä¸Šæ‰©å±•åˆ°100Bå‚æ•°çš„èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬è®¤ä¸ºFSDP + torch compileå°†æ»¡è¶³ç ”ç©¶äººå‘˜90%çš„æ‰€æœ‰éœ€æ±‚ã€‚ä½¿ç”¨linguaï¼Œæˆ‘ä»¬å°è¯•é—®ï¼š"å¾—å‡ºå…³äºæƒ³æ³•Xå¯æ‰©å±•æ€§çš„å¯é ç»“è®ºæ‰€éœ€çš„æœ€å°åŠŸèƒ½é›†æ˜¯ä»€ä¹ˆï¼Ÿ"

æˆ‘ä»¬ç›¸ä¿¡è¿™ç§æœ‰é’ˆå¯¹æ€§çš„æ–¹æ³•å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¿«åœ°å–å¾—è¿›å±•ï¼Œè€Œä¸å¿…æ‰¿æ‹…ä½¿ç”¨è®¸å¤šå¯èƒ½ä¸éœ€è¦çš„æŠ€æœ¯çš„å¿ƒç†è´Ÿæ‹…ã€‚

## Citation

```
@misc{meta_lingua,
  author = {Mathurin Videau, Badr Youbi Idrissi, Daniel Haziza, Luca Wehrstedt, Jade Copet, Olivier Teytaud, David Lopez-Paz},
  title = {{Meta Lingua}: A minimal {PyTorch LLM} training library},
  url = {https://github.com/facebookresearch/lingua},
  year = {2024}
}
```
## License

Meta Lingua ä½¿ç”¨ BSD-3-Clause è®¸å¯è¯ã€‚è¯·å‚é˜…é¡¶çº§ç›®å½•ä¸­çš„ LICENSE æ–‡ä»¶ã€‚
